1. 
   mr-jobhistory-daemon.sh start historyserver
2. 
   mr-jobhistory-daemon.sh stop historyserver

1. 简述大数据的 3V 特征
2. 简述分布式系统中的 CAP 定理
3. 简述 Hadoop 的三种运行模式及其特点
4. 简述 HDFS 中 NameNode 和 DataNode 的功能
5. 简述 MapReduce 编程模型的基本思想
6. 简述 Hbase 与传统关系数据库的差异
7. 简述 Protocol buffers 和 JSON 的差异
8. 简述对象存储和文件存储的区别
9. 简述 Spark的几种部署模式及其特点
10. 简述 Spark 和 MapReduce 编程模型的差异
11. 简述 Spark 中的弹性式数据集 RDD 的基本思想
12. 简述 Spark 中常见的 Transformation 和 Action 及其基本功能

### 1.大数据时代

三次信息化浪潮

1980前后 IBM公司制定全球个人电脑标准 ，以PC机普及为标志

1995前后  以互联网的普及为标志

2010年前后，以云计算、大数据、物联网普及为标志             **大数据时代**

**技术支撑：**

1. 存储设备

2. CPU计算能力         摩尔定律:每18-24个月 ，CPU性能提升一倍

   ​                               05年之后  单核CPU--> 多核CPU

3. 网络带宽               借助网络分布式的集群计算 

####   大数据概念 

* **Volume 大量化**   

  ​                               数据存储单位之间的换算关系

  |            单位            |    换算关系     |
  | :------------------------: | :-------------: |
  |        Byte（字节）        | 1 Byte = 8 bit  |
  |   KB（Kilobyte，千字节）   | 1KB = 1024 Byte |
  |   MB（Megabyte，兆字节）   |  1 MB = 1024KB  |
  |   GB（Gigabyte，吉字节）   |  1 GB = 1021MB  |
  | TB（Trillionbyte，太字节） | 1 TB = 1024 GB  |
  |   PB（Petabyte，拍字节）   | 1 PB = 10214 TB |
  |   EB（Exabyte，艾字节）    |   1EB= 1024PB   |
  |  ZB（Zettabyte，泽字节）   |   1ZB=1024EB    |

* **Variety 多样化**
  * 结构化数据：具有规范的行和列结构数据
  * 非结构化数据：大部分数据都是非结构化数据

* **Velocity 快速化**

     一秒定律： 从数据生成到决策响应仅需1秒

* **Value 价值密度低** 

 ### 大数据的影响

  **科学研究的四种范式**      Jim Gray

  试验、理论、计算、数据     

  **思维方式**

 全样而非抽样、效率而非精确、相关而非因果

### 大数据关键技术

**技术层面**：数据采集、数据存储与管理、数据处理与分析、数据隐私与安全

 重要在：数据存储与管理、数据处理与分析  

**两大核心技术：**

* 分布式存储

   GFS、HDFS、Big Table、HBase、NoSQL、NewSQL

* 分布式处理

​        MapReduce、Spark、Flink

### 大数据计算模式

   不同的计算模式需要使用不同的产品

1.  批处理计算 ：针对大规模数据的批量处理

    MapReduce是批处理计算模式的典型代表

    Spark

2. 流计算：针对流数据的实时计算

   流数据需要实时处理，给出实时响应

   代表产品Storm 、S4

3. 图计算：针对大规模图结构数据的处理

   社交网络数据就是图结构数据

   代表软件 Google Pregel

4. 查询分析计算：大规模的数据的存储管理和查询分析

   代表产品 Google Dremel、Hive等

### Hadoop 

不是单一的软件，是一个生态系统

Hadoop是一个由Apache基金会所开发的**<u>分布式系统</u>**基础架构，充分利用集群的威力进行高速运算和存储

Hadoop的框架最核心的设计就是：**HDFS**和**MapReduce**

HDFS（分布式文件系统）：为海量的数据提供了存储

MapReduce：为海量的数据提供了计算

YARN：提供资源调度和管理服务

Hive：数据仓库，本身不保存数据，相当于编程接口，将SQL语句自动转换对HDFS查询分析，得到结果

Pig：数据流处理

Mahout：实现常用数据挖掘算法分类、聚类、回归等

Ambari：安装、部署、配置和管理工具

Zookeeper：分布式协作服务

HBase：分布式数据库

Flume：日志收集

Sqoop：完成Hadoop系统组件之间的互通



**MapReduce**

编程容易、屏蔽底层分布式并行编程细节

策略：分而治之

Map任务+Reduce任务

**YARN**

资源调度管理框架，防止多个计算框架在同一个集群上冲突

实现底层数据的共享          

​	

**Spark**

Apache Spark 是专为大规模数据处理而设计的快速通用的**计算引擎**。

**spark生态**

Spark Core 完成RDD应用开发

Sparl SQL：分析关系数据

Spark Streaming 进行流计算

MLlib 机器学习算法库

GraphX 编写图计算应用程序

**Hadoop与Spark对比**

Hadoop的缺点在于MapReduce

1. 表达能力有限                    有些问题只用Map+Reduce结构解决不了

2. 磁盘IO开销大    将迭代结果放到磁盘中

3. 延迟高               任务之间衔接涉及IO开销，在前一个任务执行完成之前。其他任务就无法开始，难以胜任复杂、多阶段的计算任务

Spark 表达能力强、提供内存计算，中间结果放到内存中，对于迭代运算效率更高

Spark取代Hadoop？  Spark只是单纯的计算框架，Hadoop还有HDFS这一核心技术，只能取代Hadoop的MapReduce

### Flink和Beam

|          | Apache Spark                 | Apache Flink                                                 |
| -------- | ---------------------------- | ------------------------------------------------------------ |
| 核心实现 | Scala                        | Java                                                         |
| 编程接口 | Java、Python、R等语言        | DataSet API支持：Java、Scala和Python DataStreamAPI：java和Scala |
| 计算模型 | 基于数据片集合的微批处理模型 | 基于操作符的连续流模型                                       |
| 优缺点   | 流式处理有延时只支持秒级计算 | 流式计算与Storm性能相当支持毫秒级计算                        |

流计算方面的表现比Spark好，其他方面差不多

Spark比较热门原因：社区服务做得好，很多大公司加入Spark社区



Beam（Google）2017 发布      想 一统江湖  



### 2.Spark的设计与运行原理

##### 1.概述

09年由伯克利分校AMP实验室开发，13年Spark加入Apache孵化器项目后发展迅猛，如今已 成为Apache软件基金会最重要的三大分布式计算系统开源 项目之一（Hadoop、Spark、Storm）

特点：

1. 运行速度快。支持循环数据流与内存计算
2. 容易使用。支持使用Scala、Java、Python的R语言进行编程，可以使用Spark Shell进行交互式编程
3. 通用性。提供了强大的技术栈，包括SQL‘查询、流失计算、机器学习和图算法组件。
4. 运行模式多样。可运行与独立集群模式，可运行Hadoop中，也可运行在云环境中，并可以访问HDFS、HBase等多种数据源。

##### 2.Spark生态系统

​                                            大数据处理类型

| 应用类型                 | 时间跨度         |
| ------------------------ | ---------------- |
| 复杂的批量数据处理       | 数十分钟到数小时 |
| 基于历史数据的交互式查询 | 数十秒到数分钟   |
| 基于实时数据流的数据处理 | 数百毫秒到数秒   |

 同时存在以上三种场景时，就需要同时部署三种不同的软件

存在的问题：

1. 不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格 式的转换
2. 不同的软件需要不同的开发和维护团队，带来了较高的使用成本 •比较难以对同一个集群中的各个系统进行统一的资源协调和分配

Spark的设计遵循“一个软件栈满足不同应用场景”的 理念，逐渐形成了一套完整的生态系统 

* 既能够提供内存计算框架，也可以支持SQL即席查询、 实时流式计算、机器学习和图计算等 
* Spark可以部署在资源管理器YARN之上，提供一站式的 大数据解决方案 
* 因此，Spark所提供的生态系统足以应对上述三种场 景，即同时支持批处理、交互式查询

##### 基本概念

1. Resillient Distributed Dataset，RDD（弹性分布式数据集）是分布式内存的抽象概念，提供了一种高度受限的共享内存模型。    --**最核心的数据抽象**
2. Directed Acyclic Graph（有向无环图）反映RDD之间的依赖关系
3. Executor，运行在工作节点的一个进程，负责运行Task 
4. Application， 用户编写的Spark程序
5. Task  运行在Executor上工作单元
6. Job 一个job包含多个RDD及作用于相应RDD上的各种操作
7. Stage  job的基本调度单位 一个job会分为多组Task，每组Task被称为Stage，或者TaskSet，代表一组关联的、相互没有Shuffle依赖关系的任务组成的任务集 

##### RDD运行原理  

 本质上是一个只读的分区数据集合

高度受限的共享内存模型

操作类型

* Action（动作类型操作）
* Transformation（转换类型操作） map filter groupby  join

这两种操作是粗粒度修改，一次性只能针对RDD全集进行修改

Transformation 只记录轨迹，不发生计算   惰性操作

Spark提供了RDD的API，程序员可以通过调用API实现对 RDD的各种操作

